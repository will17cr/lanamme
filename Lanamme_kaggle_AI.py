{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n# %% [code]\n# %% [code]\n# coding: utf-8\n\n# Instalar librerías no estándares\nimport os\nos.system('pip install -U pip')\nos.system('pip install sickle')\nos.system('pip install dateparser')\nos.system('pip install python-dateutil')\nos.system('pip install gspread gspread_dataframe google-api-python-client')\nos.system('pip install google-generativeai requests pdfplumber')\nos.system('pip install pymupdf')\nos.system('pip install ocrmypdf')\nos.system('pip install selenium beautifulsoup4')\nos.system('pip install pytesseract Pillow') # For OCR\n# In Kaggle, ensure geckodriver:\n# os.subprocess([\"apt-get\", \"update\"],\"apt-get update\")\n# os.subprocess([\"apt-get\", \"install\", \"-y\", \"firefox\", \"geckodriver\"],\"apt-get install -y firefox geckodriver\")\n# os.subprocess([\"apt-get\",\"install\",\"-y\",\"tesseract-ocr\",\"tesseract-ocr-spa\"],\"apt-get install -y tesseract-ocr tesseract-ocr-spa\")\n\n! apt-get update -y && apt-get install -y tesseract-ocr tesseract-ocr-spa && apt-get install -y tesseract-ocr tesseract-ocr-spa\n\nprint(\"\\nScript to request records from LANAMME's repository and update our data with AI analysis\")\n\nimport datetime as dt\nimport pandas as pd\nimport numpy as np\nimport re\nimport time\nfrom datetime import datetime\nimport json\nfrom io import BytesIO\nimport fitz\n\n\nfrom sickle import Sickle\nimport gspread\ntry:\n    from gspread_dataframe import set_with_dataframe, get_as_dataframe\nexcept ImportError:\n    os.system('pip install gspread_dataframe')\n    from gspread_dataframe import set_with_dataframe, get_as_dataframe\nif os.environ.get('KAGGLE_KERNEL_RUN_TYPE') in ('Interactive', 'Batch'):\n    from kaggle_secrets import UserSecretsClient\nelse:\n    os.system('pip install python-dotenv')\n    from dotenv import load_dotenv, find_dotenv\n    if find_dotenv(): print(\"Found .env file, loading.\"); _ = load_dotenv(find_dotenv())\n    else: print(\".env file not found for local dev.\")\n\nimport google.generativeai as genai\nimport requests\nimport pdfplumber\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options as FirefoxOptions\nfrom selenium.webdriver.firefox.service import Service as FirefoxService\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urlparse, urljoin\nimport pytesseract # For OCR\nfrom PIL import Image # For OCR with Tesseract\n\n\nURL = 'https://www.lanamme.ucr.ac.cr/oai/request?'\nMASTER_SHEET_NAME = 'Master'\nRISK_CATEGORIES = [\"ninguno\", \"bajo\", \"medio\", \"alto\", \"critico\"]\n\n# NEW: Define the cutoff date for Gemini AI processing\n# Records published strictly BEFORE this date will skip Gemini analysis\n# datetime(YEAR, MONTH, DAY)\nGEMINI_PROCESSING_CUTOFF_DATE = datetime(2024, 1, 1)\n\n\n# --- SECRET LOADING ---\nif os.environ.get('KAGGLE_KERNEL_RUN_TYPE') in ('Interactive', 'Batch'):\n    print(\"Kaggle: Loading secrets.\"); user_secrets = UserSecretsClient()\n    try: GEMINI_API_KEY = user_secrets.get_secret(\"GEMINI_API_KEY\"); GOOGLE_API_SECRET = user_secrets.get_secret(\"Google_API\"); GOOGLE_SHEET_ID = user_secrets.get_secret(\"NewSheetID\"); print(\"Kaggle secrets loaded.\")\n    except Exception as e: print(f\"Error Kaggle Secrets: {e}\"); GEMINI_API_KEY, GOOGLE_API_SECRET, GOOGLE_SHEET_ID = None, None, None\nelse:\n    print(\"Local: Loading secrets from env.\");\n    try: GEMINI_API_KEY = os.environ[\"GEMINI_API_KEY\"]; GOOGLE_API_SECRET = os.environ[\"Google_API\"]; GOOGLE_SHEET_ID = os.environ[\"NewSheetID\"]; print(\"Local env secrets loaded.\")\n    except KeyError as e: print(f\"Error Local Env Var: {e} not set.\"); GEMINI_API_KEY, GOOGLE_API_SECRET, GOOGLE_SHEET_ID = None, None, None\n    except Exception as e: print(f\"Error Local Secrets: {e}\"); GEMINI_API_KEY, GOOGLE_API_SECRET, GOOGLE_SHEET_ID = None, None, None\n\n# --- GEMINI MODEL CONFIG ---\nif GEMINI_API_KEY:\n    genai.configure(api_key=GEMINI_API_KEY); GEMINI_MODEL_NAME = 'gemini-2.5-flash-preview-04-17'\n    try: generation_config = genai.types.GenerationConfig(response_mime_type=\"application/json\"); model = genai.GenerativeModel(GEMINI_MODEL_NAME, generation_config=generation_config); print(f\"Gemini model '{GEMINI_MODEL_NAME}' initialized for JSON.\")\n    except Exception as e: print(f\"Error initializing Gemini: {e}\"); model = None\nelse: model = None; print(\"Gemini API Key not found/model init failed. AI processing skipped.\")\n\n# --- HELPER FUNCTIONS ---\n\ndef get_pdf_url(url):\n\n    options = FirefoxOptions()\n    options.add_argument(\"--headless\")\n    options.add_argument(\"--disable-gpu\")\n    # options.add_argument(\"--window-size=1920,1080\")\n    \n    web=f\"{url}?show=full\"\n    \n    browser = webdriver.Firefox(options=options)\n\n    if not url or not isinstance(url, str) or not url.startswith('http'):\n        print(f\"    get_direct_pdf_url: Invalid page_url: '{url}'\")\n        return None\n\n    \n    browser.get(web)\n    time.sleep(800 / 1000)\n\n    html = browser.page_source\n    pagina = BeautifulSoup(html, 'html.parser')\n\n    pdf_url_tag = pagina.find('meta', {'name': 'citation_pdf_url'})\n    time.sleep(250 / 1000)\n    if pdf_url_tag:\n        pdf_url = pdf_url_tag['content']\n    else:\n        return None\n\n    browser.close()\n    browser.stop_client()\n    \n    return pdf_url\n\n\ndef extract_text_from_pdf_url(pdf_url):\n    \"\"\"Fetches a PDF from a URL and extracts text content.\"\"\"\n    if not pdf_url or not isinstance(pdf_url, str) or not pdf_url.lower().endswith('.pdf'):\n        print(f\"Invalid or non-PDF URL: {pdf_url}\")\n        return None\n    try:\n        response = requests.get(pdf_url, timeout=30) # Added timeout\n        response.raise_for_status() # Raises an exception for bad status codes\n        \n        text_content = \"\"\n        with BytesIO(response.content) as pdf_file:\n            with pdfplumber.open(pdf_file) as pdf:\n                for page in pdf.pages:\n                    page_text = page.extract_text()\n                    if page_text:\n                        text_content += page_text + \"\\n\"\n        return text_content.strip() if text_content else None\n    except requests.exceptions.RequestException as e:\n        print(f\"Error downloading PDF from {pdf_url}: {e}\")\n    except Exception as e:\n        print(f\"Error extracting text from PDF {pdf_url}: {e}\")\n    return None\n\ndef extract_text_from_pdf_ocrmypdf(pdf_url_direct):\n    \"\"\"\n    Fetches a PDF document from a given DIRECT PDF URL and extracts its text content\n    using PyMuPDF (fitz).\n\n    Args:\n        pdf_url_direct (str): The URL of the PDF document to process.\n\n    Returns:\n        str or None: The extracted text content from the PDF as a single string,\n                     or None if any error occurs.\n    \"\"\"\n    if not pdf_url_direct or not isinstance(pdf_url_direct, str) or not pdf_url_direct.lower().startswith('http'):\n        print(f\"  OCRmyPDF: Error - Invalid URL format for PDF extraction: '{pdf_url_direct}'\")\n        return None\n    \n    # Optional: A warning if it doesn't end with .pdf, though PyMuPDF might handle it if Content-Type is correct\n    if not pdf_url_direct.lower().endswith('.pdf'):\n        print(f\"  OCRmyPDF: Warning - Direct PDF URL does not end with .pdf: '{pdf_url_direct}'. Attempting download.\")\n\n    try:\n        print(f\"  OCRmyPDF: Attempting to download direct PDF from: {pdf_url_direct}\")\n        response = requests.get(pdf_url_direct, timeout=45, headers={'User-Agent': 'Mozilla/5.0'})\n        response.raise_for_status()  \n        \n        content_type = response.headers.get('content-type', '').lower()\n        if 'application/pdf' not in content_type:\n            print(f\"  OCRmyPDF: ERROR - Content-Type is '{content_type}', not 'application/pdf'. URL '{pdf_url_direct}' may not be a direct PDF link.\")\n            return None \n        \n        print(f\"  OCRmyPDF: Direct PDF downloaded (status {response.status_code}). Size: {len(response.content)} bytes.\")\n        \n        pdf_bytes = response.content\n        text_content = \"\"\n        \n        # Open PDF from bytes using OCRmyPDF\n        try:\n            output_buffer = BytesIO()\n            ocrmypdf.ocr(pdf_bytes, output_buffer,\n                             language=['spa'],  # Spanish language for OCR\n                             invalidate_digital_signatures=True, # process PDF even if it has digital signature\n                             force_ocr=True,\n                             deskew=True,       # Deskew pages before OCR\n                             optimize=2,        # Optimize PDF (level 3)\n                             skip_text=False      # Skip pages that already contain text\n)\n            doc = output_buffer.getvalue()\n            print(f\"  OCRmyPDF: Extracting text from {len(doc)} page(s)...\")\n            for page_num in range(len(doc)):\n                page = doc.load_page(page_num)\n                page_text = page.get_text(\"text\") # Get plain text; other options: \"html\", \"xml\", \"xhtml\", \"json\"\n                if page_text:\n                    text_content += page_text + \"\\n\"\n            doc.close()\n            if text_content:\n                print(f\"  OCRmyPDF: Text extraction complete.\")\n            else:\n                print(f\"  OCRmyPDF: No text could be extracted (PDF might be image-based or empty of text).\")\n        except Exception as e_ocrmypdf:\n            # This catches errors if fitz.open() or page.get_text() fails\n            print(f\"  OCRmyPDF: Error opening or parsing PDF with PyMuPDF from '{pdf_url_direct}': {e_ocrmypdf}\")\n            return None # Return None if PyMuPDF fails\n        \n        return text_content.strip() if text_content.strip() else None # Return None if only whitespace or empty\n\n    except requests.exceptions.Timeout:\n        print(f\"Fitz: Error downloading PDF from '{pdf_url_direct}': Request timed out.\")\n    except requests.exceptions.RequestException as e_req:\n        print(f\"Fitz: Error downloading PDF from '{pdf_url_direct}': {e_req}\")\n    except Exception as e_generic:\n        print(f\"Fitz: Generic error during PDF processing for '{pdf_url_direct}': {e_generic}\")\n    return None\n\ndef extract_text_from_pdf_fitz(pdf_url_direct):\n    \"\"\"\n    Fetches a PDF document from a given DIRECT PDF URL and extracts its text content\n    using PyMuPDF (fitz).\n\n    Args:\n        pdf_url_direct (str): The URL of the PDF document to process.\n\n    Returns:\n        str or None: The extracted text content from the PDF as a single string,\n                     or None if any error occurs.\n    \"\"\"\n    if not pdf_url_direct or not isinstance(pdf_url_direct, str) or not pdf_url_direct.lower().startswith('http'):\n        print(f\"  Fitz: Error - Invalid URL format for PDF extraction: '{pdf_url_direct}'\")\n        return None\n    \n    # Optional: A warning if it doesn't end with .pdf, though PyMuPDF might handle it if Content-Type is correct\n    if not pdf_url_direct.lower().endswith('.pdf'):\n        print(f\"  Fitz: Warning - Direct PDF URL does not end with .pdf: '{pdf_url_direct}'. Attempting download.\")\n\n    try:\n        print(f\"  Fitz: Attempting to download direct PDF from: {pdf_url_direct}\")\n        response = requests.get(pdf_url_direct, timeout=45, headers={'User-Agent': 'Mozilla/5.0'})\n        response.raise_for_status()  \n        \n        content_type = response.headers.get('content-type', '').lower()\n        if 'application/pdf' not in content_type:\n            print(f\"  Fitz: ERROR - Content-Type is '{content_type}', not 'application/pdf'. URL '{pdf_url_direct}' may not be a direct PDF link.\")\n            return None \n        \n        print(f\"  Fitz: Direct PDF downloaded (status {response.status_code}). Size: {len(response.content)} bytes.\")\n        \n        pdf_bytes = response.content\n        text_content = \"\"\n        \n        # Open PDF from bytes using PyMuPDF\n        try:\n            doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n            print(f\"  Fitz: Extracting text from {len(doc)} page(s)...\")\n            for page_num in range(len(doc)):\n                page = doc.load_page(page_num)\n                page_text = page.get_text(\"text\") # Get plain text; other options: \"html\", \"xml\", \"xhtml\", \"json\"\n                if page_text:\n                    text_content += page_text + \"\\n\"\n            doc.close()\n            if text_content:\n                print(f\"  Fitz: Text extraction complete.\")\n            else:\n                print(f\"  Fitz: No text could be extracted (PDF might be image-based or empty of text).\")\n        except Exception as e_fitz:\n            # This catches errors if fitz.open() or page.get_text() fails\n            print(f\"  Fitz: Error opening or parsing PDF with PyMuPDF from '{pdf_url_direct}': {e_fitz}\")\n            return None # Return None if PyMuPDF fails\n        \n        return text_content.strip() if text_content.strip() else None # Return None if only whitespace or empty\n\n    except requests.exceptions.Timeout:\n        print(f\"Fitz: Error downloading PDF from '{pdf_url_direct}': Request timed out.\")\n    except requests.exceptions.RequestException as e_req:\n        print(f\"Fitz: Error downloading PDF from '{pdf_url_direct}': {e_req}\")\n    except Exception as e_generic:\n        print(f\"Fitz: Generic error during PDF processing for '{pdf_url_direct}': {e_generic}\")\n    return None\n\n\ndef extract_text_from_pdf_ocr(pdf_url_direct, lang_code='spa'):\n    \"\"\"\n    Fetches a PDF from a URL, renders its pages as images, and extracts text using OCR.\n    This is a fallback method and can be slower and more resource-intensive.\n\n    Args:\n        pdf_url_direct (str): The direct URL of the PDF document.\n        lang_code (str): The language code for Tesseract OCR (e.g., 'spa' for Spanish).\n\n    Returns:\n        str or None: The extracted text content from OCR, or None if errors occur.\n    \"\"\"\n    if not pdf_url_direct or not isinstance(pdf_url_direct, str) or not pdf_url_direct.lower().startswith('http'):\n        print(f\"  OCR: Error - Invalid URL for PDF: '{pdf_url_direct}'\")\n        return None\n\n    try:\n        print(f\"  OCR: Attempting download for OCR: {pdf_url_direct}\")\n        response = requests.get(pdf_url_direct, timeout=45, headers={'User-Agent': 'Mozilla/5.0'})\n        response.raise_for_status()\n        \n        content_type = response.headers.get('content-type', '').lower()\n        if 'application/pdf' not in content_type:\n            print(f\"  OCR: ERROR - Content-Type is '{content_type}', not PDF. URL '{pdf_url_direct}'\")\n            return None\n            \n        pdf_bytes = response.content\n        print(f\"  OCR: PDF downloaded for OCR. Size: {len(pdf_bytes)} bytes.\")\n        \n        all_ocr_text = \"\"\n        doc = None\n        try:\n            doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n            print(f\"  OCR: Processing {len(doc)} pages with Tesseract (lang: {lang_code})...\")\n            for page_num in range(len(doc)):\n                page = doc.load_page(page_num)\n                # Render page to a pixmap (image). Higher DPI can improve OCR but is slower.\n                pix = page.get_pixmap(dpi=300)\n                img_bytes = pix.tobytes(\"png\") # Convert pixmap to PNG image bytes\n                \n                try:\n                    pil_image = Image.open(BytesIO(img_bytes))\n                    # Use Tesseract to extract text from the image\n                    page_ocr_text = pytesseract.image_to_string(pil_image, lang=lang_code)\n                    if page_ocr_text:\n                        all_ocr_text += page_ocr_text + \"\\n\\n\" # Add extra newline for page separation\n                    print(f\"    OCR: Page {page_num + 1} processed.\")\n                except Exception as e_ocr_page:\n                    print(f\"    OCR: Error processing page {page_num + 1} with Tesseract: {e_ocr_page}\")\n            if all_ocr_text.strip():\n                print(\"  OCR: Text extraction via OCR complete.\")\n            else:\n                print(\"  OCR: No text extracted via OCR (document might be truly empty or OCR failed on all pages).\")\n        except Exception as e_ocr_fitz:\n            print(f\"  OCR: Error opening PDF with PyMuPDF for OCR: {e_ocr_fitz}\")\n            return None # Cannot proceed if PDF can't be opened to render pages\n        finally:\n            if doc:\n                doc.close()\n        \n        return all_ocr_text.strip() if all_ocr_text.strip() else None\n\n    except requests.exceptions.Timeout:\n        print(f\"OCR: Timeout downloading PDF for OCR: '{pdf_url_direct}'\")\n    except requests.exceptions.RequestException as e_req_ocr:\n        print(f\"OCR: Request error downloading PDF for OCR: {e_req_ocr}\")\n    except Exception as e_generic_ocr:\n        print(f\"OCR: Generic error during PDF download/setup for OCR: {e_generic_ocr}\")\n    return None\n\n# Ensure RISK_CATEGORIES is defined globally or accessible in this function's scope\n# RISK_CATEGORIES = [\"ninguno\", \"bajo\", \"medio\", \"alto\", \"critico\"]\n\ndef get_gemini_analysis(document_text):\n    # Initialize default error values at the very beginning\n    default_error_rating = \"Error: AI Analysis default\"\n    default_error_explanation = \"Error: AI Explanation default\"\n    default_error_summary = \"Error: AI Summary default\"\n\n    # These variables will hold the final values to be returned\n    final_rating = default_error_rating\n    final_explanation = default_error_explanation\n    final_summary = default_error_summary\n\n    if not model or not document_text:\n        print(\"    Gemini model/text unavailable.\")\n        return final_rating, final_explanation, final_summary\n\n    # Refined prompt to emphasize stricter JSON and escaping\n    prompt_combined_json = f\"\"\"\n    Analyze the following document text, which is in Spanish.\n    Based on your analysis, generate a JSON object with the following three keys:\n    1. \"riesgo_rating\": A single string value representing the overall risk level. Choose EXCLUSIVELY from this list: {RISK_CATEGORIES}. This rating should be based on the presence, severity, and quantity of warning/alarming statements, and the extent of any danger stated.\n    2. \"riesgo_explicacion\": A detailed textual explanation IN SPANISH. Start your explanation by explicitly stating the assigned 'riesgo_rating' and then detail the primary reasons for this rating, referencing specific warnings, dangers, and concerns from the document. For example: 'El riesgo se considera [valor de riesgo_rating] debido a [razones principales y detalles específicos del documento].' Ensure any special characters like backslashes or quotes within this explanation are correctly escaped for JSON string format (e.g., a backslash should be '\\\\\\\\', a quote '\\\"').\n    3. \"resumen_detallado_ia\": A comprehensive and explanatory summary of the entire document, IN SPANISH. This summary should focus on key findings, methodologies (if applicable), conclusions, and recommendations. Ensure any special characters like backslashes or quotes within this summary are correctly escaped for JSON string format.\n\n    Ensure the output is ONLY a valid JSON object. Do not add any text before or after the JSON object. All string values inside the JSON must be properly JSON escaped. Example format:\n    {{\n      \"riesgo_rating\": \"bajo\",\n      \"riesgo_explicacion\": \"El riesgo se considera bajo porque solo se mencionaron algunas preocupaciones menores y no hay indicios de peligro inminente. Por ejemplo, una ruta de archivo podría ser C:\\\\\\\\Users\\\\\\\\temp.\",\n      \"resumen_detallado_ia\": \"El documento trata sobre la \\\\\"importancia\\\\\" de...\"\n    }}\n\n    Document Text (Spanish):\n    ---\n    {document_text[:150000]}\n    ---\n\n    JSON Output:\n    \"\"\"\n\n    print(\"    Requesting JSON (rating, explanation, summary) from Gemini...\")\n    \n    raw_text_from_gemini_for_debug = \"No response received\"\n    json_candidate_text_for_debug = \"No JSON candidate formed\"\n    \n    try:\n        response = model.generate_content(prompt_combined_json, request_options={'timeout': 180})\n\n        if not response.parts:\n            block_reason_str = \"Unknown reason\"\n            if hasattr(response, 'prompt_feedback') and response.prompt_feedback:\n                if hasattr(response.prompt_feedback, 'block_reason') and response.prompt_feedback.block_reason is not None:\n                    block_reason_str = str(response.prompt_feedback.block_reason)\n                elif hasattr(response.prompt_feedback, 'safety_ratings'):\n                    print(f\"      Safety ratings: {response.prompt_feedback.safety_ratings}\")\n                    for rating_info in response.prompt_feedback.safety_ratings:\n                        if rating_info.probability not in [genai.types.HarmProbability.NEGLIGIBLE, genai.types.HarmProbability.LOW]:\n                            block_reason_str = f\"Safety block - Category: {rating_info.category}, Probability: {rating_info.probability.name}\"\n                            break\n            print(f\"    Gemini API Warning: No content parts in response. Effective Block reason: {block_reason_str}.\")\n            final_rating = f\"AI Error: No Parts ({block_reason_str})\"\n            return final_rating, final_explanation, final_summary\n\n        raw_text_from_gemini_for_debug = response.text\n        \n        try:\n            text_to_parse = raw_text_from_gemini_for_debug.strip()\n            \n            # Attempt to find the JSON block more robustly\n            # Remove markdown backticks first if they exist\n            if text_to_parse.startswith(\"```json\"):\n                text_to_parse = text_to_parse[len(\"```json\"):]\n            if text_to_parse.endswith(\"```\"):\n                text_to_parse = text_to_parse[:-len(\"```\")]\n            text_to_parse = text_to_parse.strip() # Strip again after removing backticks\n\n            # Now, find the first '{' and last '}'\n            first_brace = text_to_parse.find('{')\n            last_brace = text_to_parse.rfind('}')\n\n            if first_brace != -1 and last_brace != -1 and last_brace > first_brace:\n                json_candidate_text = text_to_parse[first_brace : last_brace+1]\n                print(f\"    Attempting JSON parse (extracted from braces): {json_candidate_text[:100]}...\")\n            else:\n                # If no clear braces, use the cleaned text (less likely to be correct if not wrapped in braces)\n                json_candidate_text = text_to_parse\n                print(f\"    No clear JSON braces found after stripping markdown, attempting parse on: {json_candidate_text[:100]}...\")\n            \n            json_candidate_text_for_debug = json_candidate_text\n\n            try:\n                response_json = json.loads(json_candidate_text)\n            except json.JSONDecodeError as e_strict:\n                print(f\"    JSONDecodeError (standard strict parsing): {e_strict}. Trying with strict=False.\")\n                try:\n                    # This is unlikely to fix \"Invalid \\uXXXX escape\" or \"Extra data\" if the structure is wrong\n                    response_json = json.loads(json_candidate_text, strict=False)\n                    print(\"    Successfully parsed with strict=False.\")\n                except json.JSONDecodeError as e_non_strict:\n                    print(f\"    JSONDecodeError (strict=False also failed): {e_non_strict}.\")\n                    raise e_strict # Re-raise the original, more common error for outer handling\n\n            final_rating = response_json.get(\"riesgo_rating\", default_error_rating).strip().lower()\n            final_explanation = response_json.get(\"riesgo_explicacion\", default_error_explanation).strip()\n            final_summary = response_json.get(\"resumen_detallado_ia\", default_error_summary).strip()\n\n            if final_rating not in RISK_CATEGORIES and not final_rating.startswith(\"Error:\") and not final_rating.startswith(\"AI Error:\") and not final_rating.startswith(\"API Exception:\"):\n                print(f\"    Warning: Gemini returned an invalid risk category: '{final_rating}'.\")\n                final_rating = f\"Error: Invalid Category ({final_rating})\"\n            \n        except json.JSONDecodeError as e_json:\n            print(f\"    FATAL Error decoding JSON from Gemini: {e_json}.\")\n            print(f\"    --- Start of problematic text that caused JSON error (length: {len(json_candidate_text_for_debug)}) ---\")\n            # Print the exact string that caused the error, replacing non-printable characters for logging\n            printable_debug_text = ''.join(c if c.isprintable() or c.isspace() else f'\\\\x{ord(c):02x}' for c in json_candidate_text_for_debug)\n            print(printable_debug_text)\n            print(f\"    --- End of problematic text ---\")\n            final_rating = \"Error: JSON Decode\"\n            final_explanation = f\"JSON Error - {e_json}\"\n            final_summary = f\"JSON Error - {e_json}\"\n        except AttributeError as e_attr:\n            print(f\"    Error accessing Gemini response attributes (e.g., .text is None): {e_attr}\")\n            final_rating = \"Error: Response Attribute\"\n        except Exception as e_parse:\n            print(f\"    Unexpected error during Gemini response parsing: {e_parse}\")\n            final_rating = \"Error: Parse Exception\"\n\n    except Exception as e_api:\n        print(f\"    Major Error during Gemini API call or critical response issue: {type(e_api).__name__} - {e_api}\")\n        final_rating = f\"API Exception: {type(e_api).__name__}\"\n        \n    return final_rating, final_explanation, final_summary\n    \n\ndef get_dspace_data(url): # Same as before\n    print(\"Connecting to DSpace repository...\"); sickle_instance = Sickle(url, max_retries=5, timeout=60)\n    try:\n        records = sickle_instance.ListRecords(metadataPrefix='oai_dc', ignore_deleted=True); mylist = list()\n        for i, record in enumerate(records): mylist.append(record.metadata);\n        if (i + 1) % 100 == 0: print(f\"  Processed {i+1} DSpace records...\")\n        if not mylist: print(\"No records from DSpace.\"); return pd.DataFrame()\n        myDF = pd.DataFrame(mylist); print(f\"Retrieved {len(myDF)} raw DSpace records.\"); return myDF\n    except Exception as e: print(f\"Error DSpace: {e}\"); return pd.DataFrame()\n\n# MODIFIED process_dspace_records\ndef process_dspace_records(myDF_raw):\n    if myDF_raw.empty: return pd.DataFrame()\n    print(\"Processing DSpace records (Original Logic for DSpace fields)...\")\n    myDF = myDF_raw.copy()\n    myDF[\"matter\"]=False\n    myDF.loc[myDF.type.isna(),\"type\"]=myDF.loc[myDF.type.isna(),\"type\"].apply(lambda x:[\"\"])\n    myDF['tipos_str'] = [', '.join(map(str, l)) for l in myDF['type']]\n    s1=myDF[\"type\"].explode(); cond = s1.str.contains('informe', case=False, na=False)\n    myDF.loc[s1[cond].index.unique(),\"matter\"]=True; myDF=myDF[myDF.matter].copy()\n    if myDF.empty: print(\"No 'informe' records after filtering.\"); return pd.DataFrame()\n\n    myDF.loc[:,'resumen']=myDF.description.apply(lambda x: x[0] if isinstance(x, list) and x else None)\n    myDF.loc[myDF.subject.isna(),\"subject\"]=myDF.loc[myDF.subject.isna(),\"subject\"].apply(lambda x:[\"\"])\n    myDF['topicos_str'] = [', '.join(map(str, l)) for l in myDF['subject']]\n    myDF.loc[:,\"publicado_str\"]=myDF.date.apply(lambda x: x[2] if isinstance(x, list) and len(x) > 2 else (x[0] if isinstance(x, list) and len(x) > 0 else None))\n    import dateparser\n    myDF.loc[:,\"fecha_publicado\"]=myDF.publicado_str.apply(lambda x: dateparser.parse(x, settings={'PREFER_DAY_OF_MONTH': 'first', \"PREFER_MONTH_OF_YEAR\": \"first\"}) if pd.notna(x) else pd.NaT)\n    myDF['fecha_publicado'] = pd.to_datetime(myDF['fecha_publicado'], errors='coerce')\n    myDF.loc[:,\"fecha_str\"]=myDF.date.apply(lambda x: x[0] if isinstance(x, list) and x else None)\n    myDF.loc[:,'titulo']=myDF.title.apply(lambda x: x[0] if isinstance(x, list) and x else None)\n    myDF.loc[:,'title_N']=myDF.title.apply(lambda x: len(x) if isinstance(x, list) else 0) # RESTORED\n    myDF['consecutivo'] = pd.NA\n    myDF.loc[myDF.title_N!=1,\"consecutivo\"]=myDF.loc[myDF.title_N!=1,\"title\"].apply(lambda x: x[1] if isinstance(x, list) and len(x) > 1 else pd.NA) # RESTORED\n    myDF['relaciones_str'] = pd.NA\n    myDF.loc[~myDF.relation.isna(),\"relaciones_str\"]=myDF.loc[~myDF.relation.isna(),\"relation\"].apply(lambda x: x[0] if isinstance(x, list) and x else pd.NA)\n    myDF.loc[~myDF.relaciones_str.isna(),\"relaciones_str\"]=myDF.loc[~myDF.relaciones_str.isna(),\"relaciones_str\"].apply(lambda x: x.replace(\";\",\"\") if isinstance(x, str) else x)\n    fill_consecutivo_mask = myDF.consecutivo.isna() & myDF.relaciones_str.notna()\n    myDF.loc[fill_consecutivo_mask,\"consecutivo\"] = myDF.loc[fill_consecutivo_mask,\"relaciones_str\"] # RESTORED\n    myDF['autores']= myDF.creator.apply(lambda x: '; '.join(map(str, x)) if isinstance(x, list) and x else None)\n    myDF.loc[myDF.publisher.isna(),\"publisher\"]=myDF.loc[myDF.publisher.isna(),\"publisher\"].apply(lambda x:[\"\"])\n    myDF.loc[~myDF.publisher.isna(),'publicador']=myDF.loc[~myDF.publisher.isna(),\"publisher\"].apply(lambda x: x[0] if isinstance(x, list) and x else None)\n    myDF.loc[:,'formato']=myDF.format.apply(lambda x: x[0] if isinstance(x, list) and x else None)\n    myDF.loc[~myDF.language.isna(),'idioma']=myDF.loc[~myDF.language.isna(),\"language\"].apply(lambda x: x[0] if isinstance(x, list) and x else None)\n    myDF.loc[:,'enlace'] = myDF.identifier.apply(lambda x: x[-1] if isinstance(x, list) and x and isinstance(x[-1], str) and x[-1].startswith('http') else None)\n\n    # Columns to be returned by this function (original DSpace fields)\n    # Renamed some for clarity (e.g., tipos_str) to avoid potential clashes later\n    # if 'tipos' is used as a final name after more processing.\n    original_dspace_columns = [\n        'enlace', 'titulo', 'title_N', 'autores', 'fecha_publicado', 'resumen',\n        'topicos_str', 'tipos_str', 'publicador', 'formato', 'idioma', 'consecutivo',\n        'fecha_str', 'publicado_str', 'relaciones_str'\n    ]\n    finalDF_original_fields = pd.DataFrame()\n    for col_name in original_dspace_columns: # Changed col to col_name\n        if col_name in myDF.columns:\n            finalDF_original_fields[col_name] = myDF[col_name]\n        else: # Should ideally not happen if myDF processing is correct\n            finalDF_original_fields[col_name] = 0 if col_name == 'title_N' else \\\n                                                 (pd.NaT if col_name == 'fecha_publicado' else \"\")\n            print(f\"Warning: DSpace column '{col_name}' was missing, initialized default.\")\n            \n    finalDF_original_fields.sort_values(by='fecha_publicado',ascending=False,inplace=True)\n    finalDF_original_fields.reset_index(drop=True, inplace=True)\n    print(f\"{len(finalDF_original_fields)} DSpace records processed for original fields.\")\n    return finalDF_original_fields\n\ndef get_google_sheet_connection(api_secret_json_str, sheet_id_or_name): # Same as before\n    if not api_secret_json_str: print(\"Google API secret is missing.\"); return None\n    try: credentials_dict = json.loads(api_secret_json_str); gc = gspread.service_account_from_dict(credentials_dict); sh = gc.open_by_key(sheet_id_or_name); print(f\"Connected to GSheet: {sh.title}\"); return sh\n    except Exception as e: print(f\"Error connecting GSheet: {e}\"); return None\n\ndef get_master_sheet_data(sheet_connection, sheet_name): # Ensures all new columns too\n    default_ai_cols = {'riesgo': \"Not generated\", 'risk_explanation': \"Not generated\", 'resumen_IA': \"Not generated\"}\n    # ALL columns expected in the master sheet for robust handling\n    all_expected_master_cols = [\n        'enlace', 'titulo', 'title_N', 'autores', 'fecha_publicado', 'resumen',\n        'topicos_str', 'tipos_str', 'publicador', 'formato', 'idioma', 'consecutivo',\n        'fecha_str', 'publicado_str', 'relaciones_str',\n        'pdf_link_direct', 'riesgo', 'risk_explanation', 'resumen_IA'\n    ]\n    try:\n        worksheet = sheet_connection.worksheet(sheet_name); print(f\"Reading master sheet: '{sheet_name}'\")\n        df = get_as_dataframe(worksheet, evaluate_formulas=True, header=0, na_filter=False, dtype=str)\n        \n        # Ensure all expected columns exist, create if not\n        for col_exp in all_expected_master_cols:\n            if col_exp not in df.columns:\n                print(f\"Warning: Column '{col_exp}' not found in master. Creating it.\")\n                if col_exp in default_ai_cols: df[col_exp] = default_ai_cols[col_exp]\n                elif col_exp == 'title_N': df[col_exp] = \"0\" # Default as string, will convert later\n                else: df[col_exp] = \"\" # Default for other new/missing string cols\n        \n        # Specific handling for AI cols already present\n        for col, default_val in default_ai_cols.items():\n            if col in df.columns: df[col] = df[col].astype(str).replace('', default_val).fillna(default_val)\n            # else: already created above with default_val\n        \n        if 'fecha_publicado' in df.columns: df['fecha_publicado'] = pd.to_datetime(df['fecha_publicado'], errors='coerce')\n        else: df['fecha_publicado'] = pd.NaT # Should have been created by all_expected_master_cols\n        if 'title_N' in df.columns: df['title_N'] = pd.to_numeric(df['title_N'], errors='coerce').fillna(0).astype(int)\n        else: df['title_N'] = 0 # Should have been created\n\n        # Ensure other string columns are indeed string and filled\n        for col_str in ['enlace', 'pdf_link_direct', 'titulo', 'autores', 'resumen', 'topicos_str', 'tipos_str', 'publicador', 'formato', 'idioma', 'consecutivo', 'fecha_str', 'publicado_str', 'relaciones_str']:\n            if col_str in df.columns: df[col_str] = df[col_str].astype(str).fillna(\"\")\n            else: df[col_str] = \"\" # Should have been created by all_expected_master_cols\n            \n        print(f\"Read {len(df)} records from '{sheet_name}'\"); return df, worksheet\n    except gspread.exceptions.WorksheetNotFound:\n        print(f\"Master sheet '{sheet_name}' not found. Will define structure for new one.\")\n        empty_df_data = {}\n        for col in all_expected_master_cols:\n            if col == 'fecha_publicado': empty_df_data[col] = pd.Series(dtype='datetime64[ns]')\n            elif col == 'title_N': empty_df_data[col] = pd.Series(dtype='int')\n            elif col in default_ai_cols: empty_df_data[col] = default_ai_cols[col] # String \"Not generated\"\n            else: empty_df_data[col] = pd.Series(dtype='str')\n        return pd.DataFrame(empty_df_data), None\n    except Exception as e:\n        print(f\"Error reading master: {e}\"); return pd.DataFrame(), None # Simpler fallback\n\n# --- MAIN SCRIPT LOGIC ---\ndef main():\n    print(\"Script started at:\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n    if not GOOGLE_API_SECRET or not GOOGLE_SHEET_ID: print(\"Secrets missing. Exiting.\"); return\n\n    raw_dspace_df = get_dspace_data(URL)\n    if raw_dspace_df.empty: print(\"No DSpace data. Exiting.\"); return\n    base_df_from_dspace = process_dspace_records(raw_dspace_df)\n    if base_df_from_dspace.empty: print(\"No processable DSpace records. Exiting.\"); return\n\n    augmented_dspace_df = base_df_from_dspace.copy()\n    if 'pdf_link_direct' not in augmented_dspace_df.columns: augmented_dspace_df['pdf_link_direct'] = \"\"\n    if 'riesgo' not in augmented_dspace_df.columns: augmented_dspace_df['riesgo'] = \"Not generated\"\n    if 'risk_explanation' not in augmented_dspace_df.columns: augmented_dspace_df['risk_explanation'] = \"Not generated\"\n    if 'resumen_IA' not in augmented_dspace_df.columns: augmented_dspace_df['resumen_IA'] = \"Not generated\"\n    \n    gs_connection = get_google_sheet_connection(GOOGLE_API_SECRET, GOOGLE_SHEET_ID)\n    if not gs_connection: print(\"GSheet connection failed. Exiting.\"); return\n    master_df, master_worksheet = get_master_sheet_data(gs_connection, MASTER_SHEET_NAME)\n    ALL_FINAL_COLUMNS = ['enlace', 'titulo', 'title_N', 'autores', 'fecha_publicado', 'resumen', 'topicos_str', 'tipos_str', 'publicador', 'formato', 'idioma', 'consecutivo', 'fecha_str', 'publicado_str', 'relaciones_str', 'pdf_link_direct', 'riesgo', 'risk_explanation', 'resumen_IA']\n\n    if master_worksheet is None:\n        print(f\"Master sheet '{MASTER_SHEET_NAME}' creating attempt.\")\n        try:\n            expected_cols_new_sheet = ALL_FINAL_COLUMNS\n            master_worksheet = gs_connection.add_worksheet(title=MASTER_SHEET_NAME, rows=1, cols=len(expected_cols_new_sheet))\n            master_worksheet.update([expected_cols_new_sheet], 'A1')\n            print(f\"Master sheet '{MASTER_SHEET_NAME}' created with headers: {expected_cols_new_sheet}\")\n            master_df = pd.DataFrame(columns=expected_cols_new_sheet)\n            if 'fecha_publicado' in master_df.columns: master_df['fecha_publicado'] = pd.to_datetime(master_df['fecha_publicado'], errors='coerce')\n            if 'title_N' in master_df.columns: master_df['title_N'] = pd.to_numeric(master_df['title_N'], errors='coerce').fillna(0).astype(int)\n            for col_init in ALL_FINAL_COLUMNS:\n                init_val = \"Not generated\" if col_init in ['riesgo', 'risk_explanation', 'resumen_IA'] else (0 if col_init == 'title_N' else (pd.NaT if col_init == 'fecha_publicado' else \"\"))\n                if col_init not in master_df.columns: master_df[col_init] = init_val\n                else:\n                    if col_init == 'fecha_publicado': master_df[col_init] = pd.to_datetime(master_df[col_init], errors='coerce').fillna(pd.NaT)\n                    elif col_init == 'title_N': master_df[col_init] = pd.to_numeric(master_df[col_init], errors='coerce').fillna(0).astype(int)\n                    else: master_df[col_init] = master_df[col_init].astype(str).fillna(init_val)\n        except Exception as e: print(f\"Could not create master sheet: {e}. Exiting.\"); return\n\n    print(\"\\nIdentifying records for direct PDF link extraction...\")\n    for col_check_master in ['enlace', 'pdf_link_direct']:\n        if col_check_master not in master_df.columns: master_df[col_check_master] = \"\"\n        else: master_df[col_check_master] = master_df[col_check_master].astype(str).fillna(\"\")\n    existing_enlaces_in_master = set(master_df['enlace'].loc[master_df['enlace'] != ''])\n    new_dspace_records_needing_check = augmented_dspace_df[~augmented_dspace_df['enlace'].astype(str).isin(existing_enlaces_in_master)].copy()\n    enlaces_from_new_dspace_records = set(new_dspace_records_needing_check['enlace'].dropna())\n    print(f\"Found {len(enlaces_from_new_dspace_records)} 'enlace' URLs from new DSpace records for PDF link check.\")\n    enlaces_from_master_needing_pdf_link = set(master_df.loc[(master_df['pdf_link_direct'] == '') & (master_df['enlace'] != ''), 'enlace'].dropna())\n    print(f\"Found {len(enlaces_from_master_needing_pdf_link)} 'enlace' URLs in Master for PDF link check.\")\n    unique_enlaces_to_fetch_pdf_for = enlaces_from_new_dspace_records.union(enlaces_from_master_needing_pdf_link)\n    print(f\"Total unique landing pages to process for PDF links: {len(unique_enlaces_to_fetch_pdf_for)}\")\n    enlace_to_direct_pdf_map = {}\n    if unique_enlaces_to_fetch_pdf_for:\n        print(\"Starting Selenium/BS4 for direct PDF links...\")\n        processed_landing_count = 0\n        for i, landing_url in enumerate(list(unique_enlaces_to_fetch_pdf_for)):\n            print(f\"  Processing landing page {i+1}/{len(unique_enlaces_to_fetch_pdf_for)}: {landing_url}\")\n            if pd.notna(landing_url) and isinstance(landing_url, str) and landing_url.startswith('http'):\n                try:\n                    direct_pdf_url = get_pdf_url(landing_url)\n                    enlace_to_direct_pdf_map[landing_url] = direct_pdf_url if direct_pdf_url else \"\"\n                    if direct_pdf_url: print(f\"    Mapped '{landing_url}' to direct PDF: '{direct_pdf_url}'\")\n                    else: print(f\"    Could not extract PDF URL for '{landing_url}'.\")\n                    processed_landing_count +=1\n                    if processed_landing_count > 0 and processed_landing_count % 5 == 0:\n                        print(f\"    Pausing after {processed_landing_count} pages...\"); time.sleep(3)\n                except Exception as e_sel_main:\n                    print(f\"    Error get_pdf_url for '{landing_url}': {e_sel_main}\");\n                    enlace_to_direct_pdf_map[landing_url] = \"\"\n            else: enlace_to_direct_pdf_map[landing_url] = \"\"\n        print(\"Finished Selenium/BS4 processing.\")\n    if 'pdf_link_direct' not in augmented_dspace_df.columns:\n        augmented_dspace_df['pdf_link_direct'] = \"\"\n    augmented_dspace_df['pdf_link_direct'] = augmented_dspace_df['enlace'].map(enlace_to_direct_pdf_map).fillna(augmented_dspace_df['pdf_link_direct'])\n    if not master_df.empty and enlace_to_direct_pdf_map:\n        update_mask_master = (master_df['pdf_link_direct'] == '') & (master_df['enlace'].isin(enlace_to_direct_pdf_map.keys()))\n        mapped_values_master = master_df.loc[update_mask_master, 'enlace'].map(enlace_to_direct_pdf_map)\n        if not mapped_values_master.empty: master_df.loc[update_mask_master, 'pdf_link_direct'] = mapped_values_master.fillna(\"\");\n        print(\"Updated 'pdf_link_direct' in master_df.\")\n\n    if master_df.empty or not existing_enlaces_in_master: new_records_df_for_append = augmented_dspace_df.copy()\n    else: new_records_df_for_append = augmented_dspace_df[~augmented_dspace_df['enlace'].astype(str).isin(existing_enlaces_in_master)].copy()\n    print(f\"Final count of new records for appending: {len(new_records_df_for_append)}\")\n    \n    if not new_records_df_for_append.empty:\n        print(f\"Processing {len(new_records_df_for_append)} new records for sheets...\")\n        df_to_write_dated_sheet = new_records_df_for_append.copy()\n        for idx, row_new in df_to_write_dated_sheet.iterrows():\n            pub_date = row_new.get('fecha_publicado')\n            if pd.notna(pub_date) and hasattr(pub_date, 'year') and (pub_date < GEMINI_PROCESSING_CUTOFF_DATE):\n                df_to_write_dated_sheet.loc[idx, ['riesgo', 'risk_explanation', 'resumen_IA']] = \"Not processed (Old)\"\n        date_str = datetime.today().strftime('%Y-%m-%d'); new_sheet_title = f'New_{date_str}'\n        try:\n            cols_for_dated_sheet = ALL_FINAL_COLUMNS\n            final_df_for_dated_sheet = df_to_write_dated_sheet.reindex(columns=cols_for_dated_sheet).copy()\n            try: new_records_worksheet = gs_connection.worksheet(new_sheet_title); new_records_worksheet.clear(); print(f\"Cleared dated sheet: '{new_sheet_title}'.\")\n            except gspread.exceptions.WorksheetNotFound: new_records_worksheet = gs_connection.add_worksheet(title=new_sheet_title, rows=max(1, len(final_df_for_dated_sheet) + 1), cols=len(cols_for_dated_sheet))\n            for col_dt in final_df_for_dated_sheet.select_dtypes(include=['datetime64[ns]']).columns:\n                final_df_for_dated_sheet[col_dt] = final_df_for_dated_sheet[col_dt].apply(lambda x: x.isoformat() if pd.notnull(x) and hasattr(x, 'isoformat') else \"\")\n            final_df_for_dated_sheet = final_df_for_dated_sheet.fillna('')\n            set_with_dataframe(new_records_worksheet, final_df_for_dated_sheet, include_index=False, resize=True)\n            print(f\"Saved {len(final_df_for_dated_sheet)} new records to dated sheet: '{new_sheet_title}'\")\n            df_to_append_master_gs = df_to_write_dated_sheet.copy()\n            master_df_before_append_len = len(master_df)\n            try:\n                master_headers = master_worksheet.row_values(1) if master_worksheet.row_count > 0 else ALL_FINAL_COLUMNS\n                df_prep_gs_final = df_to_append_master_gs.reindex(columns=master_headers).copy()\n                for col_dt_gs in df_prep_gs_final.select_dtypes(include=['datetime64[ns]']).columns:\n                    df_prep_gs_final[col_dt_gs] = df_prep_gs_final[col_dt_gs].apply(lambda x: x.isoformat() if pd.notnull(x) and hasattr(x, 'isoformat') else \"\")\n                df_prep_gs_final = df_prep_gs_final.fillna('')\n                df_for_gs_append_final_vals = df_prep_gs_final.values.tolist()\n                if df_for_gs_append_final_vals: print(f\"Appending {len(df_for_gs_append_final_vals)} rows to master.\"); master_worksheet.append_rows(df_for_gs_append_final_vals, value_input_option='USER_ENTERED'); print(\"Appended to master.\")\n                else: print(\"No data to append to master.\")\n                temp_master_df = master_df.copy(); temp_df_to_append = df_to_write_dated_sheet.copy()\n                all_cols_concat = list(set(temp_master_df.columns) | set(temp_df_to_append.columns));\n                if not all_cols_concat: all_cols_concat = ALL_FINAL_COLUMNS\n                temp_master_df = temp_master_df.reindex(columns=all_cols_concat); temp_df_to_append = temp_df_to_append.reindex(columns=all_cols_concat)\n                master_df = pd.concat([temp_master_df, temp_df_to_append], ignore_index=True)\n                if 'fecha_publicado' in master_df.columns: master_df['fecha_publicado'] = pd.to_datetime(master_df['fecha_publicado'], errors='coerce')\n                if 'title_N' in master_df.columns: master_df['title_N'] = pd.to_numeric(master_df['title_N'], errors='coerce').fillna(0).astype(int)\n                for col_ai_c in ALL_FINAL_COLUMNS:\n                    init_val_c = \"Not generated\" if col_ai_c in ['riesgo', 'risk_explanation', 'resumen_IA'] else (0 if col_ai_c == 'title_N' else (pd.NaT if col_ai_c == 'fecha_publicado' else \"\"))\n                    if col_ai_c not in master_df.columns: master_df[col_ai_c] = init_val_c\n                    else:\n                        if col_ai_c == 'fecha_publicado': master_df[col_ai_c] = pd.to_datetime(master_df[col_ai_c], errors='coerce').fillna(pd.NaT)\n                        elif col_ai_c == 'title_N': master_df[col_ai_c] = pd.to_numeric(master_df[col_ai_c], errors='coerce').fillna(0).astype(int)\n                        elif col_ai_c in ['riesgo', 'risk_explanation', 'resumen_IA']: master_df[col_ai_c] = master_df[col_ai_c].astype(str).fillna(\"Not generated\")\n                        else: master_df[col_ai_c] = master_df[col_ai_c].astype(str).fillna(\"\")\n                print(f\"In-memory master_df updated. Length: {len(master_df)} (was {master_df_before_append_len})\")\n            except Exception as e_append: print(f\"ERROR during append/master_df update: {e_append}\")\n        except Exception as e_dated: print(f\"Error saving to dated sheet: {e_dated}\")\n    else: print(\"No new records to add.\")\n\n    if not GEMINI_API_KEY or model is None: print(\"Gemini API not configured. Skipping AI.\")\n    else:\n        print(\"\\nStarting AI Analysis...\"); master_df_updated = master_df.copy()\n        for col_ai_upd in ['riesgo', 'risk_explanation', 'resumen_IA']:\n            if col_ai_upd not in master_df_updated.columns: master_df_updated[col_ai_upd] = \"Not generated\"\n            else: master_df_updated[col_ai_upd] = master_df_updated[col_ai_upd].astype(str).replace('', \"Not generated\").fillna(\"Not generated\")\n        if 'pdf_link_direct' not in master_df_updated.columns: master_df_updated['pdf_link_direct'] = \"\"\n        else: master_df_updated['pdf_link_direct'] = master_df_updated['pdf_link_direct'].astype(str).fillna(\"\")\n        processed_for_ai_count = 0\n        records_to_process_mask = (master_df_updated['riesgo'].astype(str).str.strip().str.lower() == \"not generated\") | (master_df_updated['risk_explanation'].astype(str).str.strip().str.lower() == \"not generated\") | (master_df_updated['resumen_IA'].astype(str).str.strip().str.lower() == \"not generated\")\n        records_to_process_indices = master_df_updated[records_to_process_mask].index\n        print(f\"Found {len(records_to_process_indices)} records for AI. Checking constraints...\")\n        for index_loop in records_to_process_indices:\n            row_loop = master_df_updated.loc[index_loop]; doc_title_loop = str(row_loop.get('titulo', 'N/A')); actual_pdf_to_process = str(row_loop.get('pdf_link_direct', '')).strip(); publish_date_loop = row_loop.get('fecha_publicado')\n            if pd.isna(publish_date_loop): master_df_updated.loc[index_loop, ['riesgo', 'risk_explanation', 'resumen_IA']] = \"Not processed (Missing Date)\"; continue\n            if not (isinstance(publish_date_loop, pd.Timestamp) or isinstance(publish_date_loop, datetime)):\n                try: publish_date_loop = pd.to_datetime(publish_date_loop)\n                except: pass\n            if not (isinstance(publish_date_loop, pd.Timestamp) or isinstance(publish_date_loop, datetime)) or pd.isna(publish_date_loop): master_df_updated.loc[index_loop, ['riesgo', 'risk_explanation', 'resumen_IA']] = \"Not processed (Bad Date)\"; continue\n            if publish_date_loop < GEMINI_PROCESSING_CUTOFF_DATE: master_df_updated.loc[index_loop, ['riesgo', 'risk_explanation', 'resumen_IA']] = \"Not processed (Old)\"; continue\n            print(f\"\\nProcessing AI for Idx {index_loop} (T: {doc_title_loop}, Date: {publish_date_loop.strftime('%Y-%m-%d')}) Direct PDF: '{actual_pdf_to_process}'\")\n            if not actual_pdf_to_process or not actual_pdf_to_process.lower().startswith('http'):\n                print(f\"  Invalid direct PDF link ('{actual_pdf_to_process}').\");\n                for fld_ai in ['riesgo', 'risk_explanation', 'resumen_IA']:\n                    if str(master_df_updated.loc[index_loop, fld_ai]).strip().lower() == \"not generated\": master_df_updated.loc[index_loop, fld_ai] = \"Invalid Direct PDF Link\"\n                continue\n            \n            document_text = None; print(f\"  Attempting text extraction with PyMuPDF (fitz) for '{actual_pdf_to_process}'...\")\n            document_text = extract_text_from_pdf_ocrmypdf(actual_pdf_to_process)\n            if not document_text:\n                print(f\"  PyMuPDF (fitz) failed. Attempting OCR fallback for '{actual_pdf_to_process}'...\")\n                try:\n                    pytesseract.get_tesseract_version(); document_text = extract_text_from_pdf_ocr(actual_pdf_to_process, lang_code='spa')\n                    if document_text: print(\"  OCR fallback extracted text.\")\n                    else: print(\"  OCR fallback also failed.\")\n                except Exception as e_tess: print(f\"  OCR fallback skipped: Tesseract not working: {e_tess}\")\n            \n            if document_text:\n                print(f\"  Extracted text (len {len(document_text)}). Gemini...\"); riesgo_cat_val, riesgo_expl_val, resumen_ia_val = get_gemini_analysis(document_text)\n                if str(master_df_updated.loc[index_loop, 'riesgo']).strip().lower() == \"not generated\": master_df_updated.loc[index_loop, 'riesgo'] = riesgo_cat_val\n                if str(master_df_updated.loc[index_loop, 'risk_explanation']).strip().lower() == \"not generated\": master_df_updated.loc[index_loop, 'risk_explanation'] = riesgo_expl_val\n                if str(master_df_updated.loc[index_loop, 'resumen_IA']).strip().lower() == \"not generated\": master_df_updated.loc[index_loop, 'resumen_IA'] = resumen_ia_val\n                print(f\"    R: {master_df_updated.loc[index_loop, 'riesgo']}, Expl: {master_df_updated.loc[index_loop, 'risk_explanation'][:50]}..., SumIA: {master_df_updated.loc[index_loop, 'resumen_IA'][:50]}...\")\n                processed_for_ai_count +=1\n            else:\n                print(f\"  Failed text extraction from '{actual_pdf_to_process}'.\");\n                for fld_ai_fail in ['riesgo', 'risk_explanation', 'resumen_IA']:\n                    if str(master_df_updated.loc[index_loop, fld_ai_fail]).strip().lower() == \"not generated\": master_df_updated.loc[index_loop, fld_ai_fail] = \"PDF Text Extraction Failed (All Methods)\"\n            if processed_for_ai_count > 0 and processed_for_ai_count % 3 == 0 : print(\"  Pausing (3 docs)...\"); time.sleep(5)\n        if processed_for_ai_count > 0 or len(records_to_process_indices) > 0:\n            print(f\"\\nAI processing finished. {processed_for_ai_count} to Gemini. Updating master...\");\n            try:\n                master_headers_final = master_worksheet.row_values(1) if master_worksheet.row_count > 0 else ALL_FINAL_COLUMNS\n                final_df_to_upload = master_df_updated.copy().reindex(columns=master_headers_final)\n                for col_final_dt_upd in final_df_to_upload.select_dtypes(include=['datetime64[ns]']).columns:\n                    final_df_to_upload[col_final_dt_upd] = final_df_to_upload[col_final_dt_upd].apply(lambda x: x.isoformat() if pd.notnull(x) and hasattr(x, 'isoformat') else \"\")\n                final_df_to_upload = final_df_to_upload.fillna('')\n                set_with_dataframe(master_worksheet, final_df_to_upload, include_index=False, resize=True); print(\"Master sheet updated.\")\n            except Exception as e_master_upd: print(f\"Error updating master sheet: {e_master_upd}\")\n        else: print(\"No records required AI updates.\")\n    print(\"\\nScript finished at:\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n\nif __name__ == '__main__':\n    main()","metadata":{"_uuid":"aa9a245b-2b93-4e98-b8b3-c159cdfb45f1","_cell_guid":"48471c88-3752-4522-9897-ffcb722e0c6a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}